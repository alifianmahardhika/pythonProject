# -*- coding: utf-8 -*-
"""dicoding-submission-NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19c3JIYeTt77qHI-hDUFmVnMCU1VyYWdp

# Data

### Load data
"""

import pandas as pd
df = pd.read_csv('/content/dataset-twitter-ite.csv')
df.head()

"""### Pre-processing data"""

ite_labels = pd.get_dummies(df.sentimen)
df_new = pd.concat([df, ite_labels], axis=1)
df_new = df_new.drop(columns='sentimen')
df_new.columns = ['Twit', 'Netral', 'Positif', 'Negatif', 'ITE1', 'ITE2', 'ITE3', 'ITE4']
df_new

"""### Tokenizing texts"""

tweets = df_new['Twit'].values
label = df_new[['ITE1', 'ITE2', 'ITE3', 'ITE4']].values
from sklearn.model_selection import train_test_split
tweet_train, tweet_test, label_train, label_test = train_test_split(tweets, label, test_size=0.2)
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(tweet_train)
tokenizer.fit_on_texts(tweet_test)
sequences_train = tokenizer.texts_to_sequences(tweet_train)
sequences_test = tokenizer.texts_to_sequences(tweet_test)
padded_train = pad_sequences(sequences_train)
padded_test = pad_sequences(sequences_test)
print(padded_test.shape, label_test.shape)

"""## Model

### Creating callbacks
"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      print("\nAkurasi telah mencapai >90%!")
      self.model.stop_training = True
callbacks = myCallback()

"""### Find the best optimizer"""

from matplotlib import pyplot
from time import time
import tensorflow as tf

def model_fit_optimizer(train_input, train_output, test_input, test_output, optimizer):
  start_time = time()
  # create model
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(4, activation='softmax')
    ])
  # compile model
  model.compile(loss='categorical_crossentropy',
                optimizer=optimizer,
                metrics=['accuracy'])
  # fit model
  model_history = model.fit(
                    train_input,
                    train_output,
                    epochs=10,
                    validation_data=(test_input, test_output),
                    verbose=0)
  # time to train model
  finish_time = time()-start_time
  print('Time to train with optimizer ' + optimizer, finish_time)
  # evaluate model
  # eval_results = model.evaluate(train_generator, batch_size=batch_size)
  # print('Model accuracy with optimizer ' + optimizer, eval_results[1])
  # plot learning curves
  pyplot.plot(model_history.history['accuracy'], label='train')
  pyplot.plot(model_history.history['val_accuracy'], label='test')
  pyplot.ylim(bottom=0.0, top=1.0)
  pyplot.xlabel('epoch')
  pyplot.ylabel('accuracy')
  pyplot.legend(['train', 'test'], loc='lower right')
  pyplot.title('optimizer = '+optimizer, pad=-100)

"""## Plotting accuracy for each optimizers"""

# create learning curves for different optimizers
optimizers = ['sgd', 'rmsprop', 'adagrad', 'adam']
for i in range(len(optimizers)):
	# plot position
	plot_no = 220 + (i+1)
	pyplot.subplot(plot_no)
	# fit, evaluate model, determine time to train, and plot learning curves for an optimizer
	model_fit_optimizer(padded_train, label_train, padded_test, label_test, optimizers[i])
# show learning curves
pyplot.tight_layout()
pyplot.show()

"""# Final results"""

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(4, activation='softmax')
    ])
  # compile model
model.compile(loss='categorical_crossentropy',
                optimizer='sgd',
                metrics=['accuracy'])
  # fit model
model_history = model.fit(
                    padded_train,
                    label_train,
                    epochs=10,
                    validation_data=(padded_test, label_test),
                    verbose=2, callbacks=[callbacks])

