# -*- coding: utf-8 -*-
"""dicoding-submission-NLP-3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-dzG8VG2ONn6uH4_l30H-VAZLdBVHp07

# Load Libraries
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import os

"""# Data

## Load dataset dan membagi 80% training 20% test/validation
"""

(train_set, val_set), info = tfds.load('imagenette/160px', split=['train[:80%]', 'train[:20%]'], with_info=True, as_supervised=True)

"""### Preview dataset"""

fig = tfds.show_examples(train_set, info)

"""## Jumlah dataset"""

print( "Jumlah dataset = ", len(train_set) + len(val_set))

"""## Resolusi dan kelas dalam 5 contoh dari 10315 train data dan 2579 test dataset"""

ds_example = train_set.take(5)
for image, label in ds_example:  # example is (image, label)
  print(image.shape, label)

class_number = info.features["label"].num_classes
print(class_number)

get_label_name = info.features['label'].int2str

image, label = next(iter(train_set))
_ = plt.imshow(image)
_ = plt.title(get_label_name(label))

"""### Pre-processing data"""

IMG_SIZE = 100
AUTOTUNE = tf.data.AUTOTUNE
batch_size = 64

resize_and_rescale = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.Resizing(IMG_SIZE, IMG_SIZE),
  tf.keras.layers.experimental.preprocessing.Rescaling(1./255)
])

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
])


def prepare(ds, shuffle=False, is_Training=False):
  # Resize and rescale all datasets
  ds = ds.map(lambda x, y: (resize_and_rescale(x), y), 
              num_parallel_calls=AUTOTUNE)

  # Batch all datasets
  ds = ds.batch(batch_size)

  # Use data augmentation only on the training set
  if is_Training:
    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), 
                num_parallel_calls=AUTOTUNE)
    ds = ds.shuffle(10000)
    ds = ds.repeat()

  # Use buffered prefecting on all datasets
  return ds.prefetch(buffer_size=AUTOTUNE)

result = resize_and_rescale(image)
_ = plt.imshow(result)

print("Min and max pixel values:", result.numpy().min(), result.numpy().max())

train_ds = prepare(train_set, is_Training=True)
val_ds = prepare(val_set)

"""## Model

## Creating callbacks
"""

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    print('\nLearning rate for epoch {} is {}'.format(epoch + 1,
                                                      model.optimizer.lr.numpy()))
    if(logs.get('accuracy') > 0.92):
      print("\n Akurasi > 92%!")
      self.model.stop_training = True

"""## Create, Train, Test Model with GPU"""

strategy = tf.distribute.MirroredStrategy()

from tensorflow.keras.layers import Input
from tensorflow.keras.applications import VGG16

with strategy.scope():
  model = tf.keras.Sequential([
    VGG16(weights="imagenet", include_top=False, input_tensor=Input(shape=(IMG_SIZE, IMG_SIZE, 3))),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Flatten(), 
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(class_number)
  ])

  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),
                metrics=['accuracy'])

checkpoint_dir = './training_checkpoints'
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir='./logs'),
    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
                                       save_weights_only=True),
    myCallback()
]

steps_per_epoch = 6000 // batch_size
validation_steps = 1000 // batch_size

model.layers[0].trainable = False
model_history = model.fit(train_ds,
                          epochs=50,
                          steps_per_epoch=steps_per_epoch,
                          validation_data=val_ds, 
                          validation_steps=validation_steps,
                          callbacks=callbacks)

"""# Plot Akurasi dan Loss"""

print("Akurasi model (Train) =", model_history.history['accuracy'][-1])
print("Akurasi model (Test) =", model_history.history['val_accuracy'][-1])
plt.plot(model_history.history['accuracy'], label='train')
plt.plot(model_history.history['val_accuracy'], label='test')
plt.ylim(bottom=0.0, top=1.0)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend(['train', 'test'], loc='lower right')
plt.title('Plot of accuracy', pad=-100)

print("Loss model (Train) =", model_history.history['accuracy'][-1])
print("Loss model (Test) =", model_history.history['val_accuracy'][-1])
plt.plot(model_history.history['loss'], label='train')
plt.plot(model_history.history['val_loss'], label='test')
plt.ylim(bottom=0.0, top=1.0)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'test'], loc='lower right')
plt.title('Plot of loss', pad=-100)

"""# Konversi Model ke TF-Lite"""

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)

